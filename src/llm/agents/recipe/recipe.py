from pathlib import Path

from langchain_core.messages import SystemMessage, HumanMessage

from src.db.recipe.repository import get_recipes
from src.llm.agents.common.base_agent import BaseLLMAgent
from src.llm.agents.recipe.schemas import RecipeStructuredSchema
from src.llm.graph_schema import AgentState
from src.logger.logger import logger
from src.redis_client.client import cache
from src.redis_client.services import make_cache_key


class RecipeAgent(BaseLLMAgent):
    """
    RecipeAgent is responsible for selecting suitable recipes
    based on a plan generated by the PlannerAgent using an LLM.
    """

    def __init__(self) -> None:
        """
        Initialize the RecipeAgent with the appropriate LLM client.
        """
        super().__init__()

    async def create_prompt(self, plan: str) -> str:
        """
        Construct the prompt for the LLM based on the provided plan and available recipes.

        Args:
            plan (str): The plan generated by the planner agent.

        Returns:
            str: The constructed prompt to be sent to the LLM.
        """
        prompt = await self.get_prompt(agent_name=Path(__file__).parent.name)
        recipes = await get_recipes()
        logger.info(f"Available recipes: {recipes}")

        messages = [
            SystemMessage(content=prompt),
            HumanMessage(content=f"Planner agent plan: {plan}"),
            SystemMessage(content=f"Recipes: {recipes}")
        ]

        final_request = "\n".join([message.content for message in messages])
        logger.info(f"RecipeAgent prompt: {final_request}")
        return final_request

    async def generate(self, user_input: AgentState) -> AgentState:
        """
        Generate a list of recipes based on the provided plan using the LLM
        and update the agent state.

        Args:
            user_input (AgentState): The current agent state containing the plan.

        Returns:
            AgentState: The updated agent state with selected recipes.
        """
        plan = user_input.get("plan", "")
        cache_key = make_cache_key("plan", plan)

        cached = await cache.get(cache_key)
        if cached:
            logger.info(f"RecipeAgent cache hit: {cache_key}")
            user_input["recipes"] = cached
            return user_input

        prompt = await self.create_prompt(plan=plan)
        llm_with_structured_output = self.llm.with_structured_output(RecipeStructuredSchema)
        llm_response = await llm_with_structured_output.ainvoke(prompt)

        logger.info(f"RecipeAgent response: {llm_response}")

        recipes = llm_response.names
        await cache.set(cache_key, recipes)

        user_input["recipes"] = recipes
        return user_input
